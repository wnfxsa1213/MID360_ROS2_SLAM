#!/usr/bin/env python3
"""
æµ‹è¯•ä¼˜åŒ–åçš„åœ°å›¾ç²¾ç»†åŒ–é…ç½®

ç”¨äºéªŒè¯æ–°é…ç½®æ˜¯å¦èƒ½è§£å†³æ— å»ºç­‘ç‰©åŒºåŸŸå™ªç‚¹é—®é¢˜
"""

import os
import sys
import argparse
import time
from pathlib import Path

# ç›´æ¥è¿è¡Œä¸»æ¨¡å—
import subprocess
import numpy as np
import open3d as o3d

def analyze_point_density_distribution(pcd_path: str, name: str = ""):
    """åˆ†æç‚¹äº‘å¯†åº¦åˆ†å¸ƒ"""
    print(f"\n=== {name} å¯†åº¦åˆ†æ ===")

    pcd = o3d.io.read_point_cloud(pcd_path)
    points = np.asarray(pcd.points)

    if len(points) == 0:
        print("ç‚¹äº‘ä¸ºç©º")
        return

    print(f"æ€»ç‚¹æ•°: {len(points):,}")

    # è®¡ç®—è¾¹ç•Œæ¡†
    min_bound = np.min(points, axis=0)
    max_bound = np.max(points, axis=0)
    dimensions = max_bound - min_bound
    volume = np.prod(dimensions)

    print(f"è¾¹ç•Œæ¡†: {dimensions}")
    print(f"ä½“ç§¯: {volume:.2f} ç«‹æ–¹ç±³")
    print(f"æ•´ä½“å¯†åº¦: {len(points)/volume:.2f} ç‚¹/ç«‹æ–¹ç±³")

    # åˆ†æç©ºé—´åˆ†å¸ƒ
    # å°†ç©ºé—´åˆ†æˆç½‘æ ¼ï¼Œåˆ†ææ¯ä¸ªç½‘æ ¼çš„å¯†åº¦
    grid_size = 1.0  # 1ç±³ç½‘æ ¼

    x_bins = int(np.ceil(dimensions[0] / grid_size))
    y_bins = int(np.ceil(dimensions[1] / grid_size))
    z_bins = int(np.ceil(dimensions[2] / grid_size))

    # è®¡ç®—æ¯ä¸ªç‚¹æ‰€åœ¨çš„ç½‘æ ¼ç´¢å¼•
    grid_indices = np.floor((points - min_bound) / grid_size).astype(int)

    # ç»Ÿè®¡æ¯ä¸ªç½‘æ ¼çš„ç‚¹æ•°
    grid_counts = {}
    for i, (x, y, z) in enumerate(grid_indices):
        # ç¡®ä¿ç´¢å¼•åœ¨èŒƒå›´å†…
        x = min(x, x_bins - 1)
        y = min(y, y_bins - 1)
        z = min(z, z_bins - 1)

        grid_key = (x, y, z)
        grid_counts[grid_key] = grid_counts.get(grid_key, 0) + 1

    # è®¡ç®—ç½‘æ ¼å¯†åº¦ç»Ÿè®¡
    densities = list(grid_counts.values())
    total_grids = x_bins * y_bins * z_bins
    empty_grids = total_grids - len(grid_counts)

    print(f"\nç½‘æ ¼åˆ†æ (1mÂ³ç½‘æ ¼):")
    print(f"æ€»ç½‘æ ¼æ•°: {total_grids:,}")
    print(f"éç©ºç½‘æ ¼æ•°: {len(grid_counts):,}")
    print(f"ç©ºç½‘æ ¼æ•°: {empty_grids:,}")
    print(f"ç©ºç½‘æ ¼æ¯”ä¾‹: {empty_grids/total_grids*100:.1f}%")

    if densities:
        print(f"éç©ºç½‘æ ¼å¯†åº¦ç»Ÿè®¡:")
        print(f"  å¹³å‡å¯†åº¦: {np.mean(densities):.2f} ç‚¹/ç½‘æ ¼")
        print(f"  å¯†åº¦æ ‡å‡†å·®: {np.std(densities):.2f}")
        print(f"  æœ€å°å¯†åº¦: {np.min(densities)} ç‚¹/ç½‘æ ¼")
        print(f"  æœ€å¤§å¯†åº¦: {np.max(densities)} ç‚¹/ç½‘æ ¼")
        print(f"  ä¸­ä½æ•°å¯†åº¦: {np.median(densities):.2f} ç‚¹/ç½‘æ ¼")

        # æ‰¾å‡ºä½å¯†åº¦ç½‘æ ¼ï¼ˆå¯èƒ½çš„å™ªå£°åŒºåŸŸï¼‰
        low_density_threshold = np.percentile(densities, 25)  # 25%åˆ†ä½æ•°
        low_density_grids = sum(1 for d in densities if d <= low_density_threshold)
        print(f"  ä½å¯†åº¦ç½‘æ ¼æ•°é‡ (<= {low_density_threshold:.0f} ç‚¹): {low_density_grids}")

def test_optimized_refinement():
    """æµ‹è¯•ä¼˜åŒ–åçš„ç²¾ç»†åŒ–é…ç½®"""

    # æŸ¥æ‰¾æµ‹è¯•ç”¨çš„åœ°å›¾æ–‡ä»¶
    saved_maps_dir = Path("/home/tianyu/codes/Mid360map/saved_maps")

    # æŸ¥æ‰¾åŸå§‹PCDæ–‡ä»¶
    original_files = list(saved_maps_dir.glob("*[!_refined].pcd"))

    if not original_files:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹PCDæ–‡ä»¶è¿›è¡Œæµ‹è¯•")
        return False

    # ä½¿ç”¨æœ€æ–°çš„æ–‡ä»¶è¿›è¡Œæµ‹è¯•
    original_file = max(original_files, key=os.path.getmtime)
    print(f"âœ… ä½¿ç”¨æµ‹è¯•æ–‡ä»¶: {original_file}")

    # åˆ†æåŸå§‹æ–‡ä»¶å¯†åº¦åˆ†å¸ƒ
    analyze_point_density_distribution(str(original_file), "åŸå§‹åœ°å›¾")

    # è®¾ç½®è¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = saved_maps_dir / f"{original_file.stem}_optimized_refined.pcd"

    # ä½¿ç”¨ä¼˜åŒ–é…ç½®è¿›è¡Œå¤„ç†
    config_path = Path(__file__).parent / "map_refinement/config/refinement_config_optimized.yaml"

    print(f"\nğŸ”§ å¼€å§‹ä½¿ç”¨ä¼˜åŒ–é…ç½®è¿›è¡Œç²¾ç»†åŒ–å¤„ç†...")
    print(f"é…ç½®æ–‡ä»¶: {config_path}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_file}")

    try:
        # åˆå§‹åŒ–å¤„ç†ç®¡é“
        pipeline = MapRefinementPipeline(str(config_path))

        # æ‰§è¡Œå¤„ç†
        start_time = time.time()
        result = pipeline.process_single_cloud(
            str(original_file),
            str(output_file),
            pipeline="custom",  # ä½¿ç”¨è‡ªå®šä¹‰ç®¡é“
            save_intermediate=True
        )
        processing_time = time.time() - start_time

        if result['success']:
            print(f"âœ… å¤„ç†æˆåŠŸå®Œæˆ!")
            print(f"å¤„ç†æ—¶é—´: {processing_time:.1f}s")
            print(f"ç‚¹æ•°å˜åŒ–: {result['original_points']:,} -> {result['final_points']:,}")
            print(f"ç‚¹æ•°å˜åŒ–ç‡: {result['point_change_ratio']*100:.2f}%")

            # åˆ†æå¤„ç†åçš„å¯†åº¦åˆ†å¸ƒ
            analyze_point_density_distribution(str(output_file), "ä¼˜åŒ–ç²¾ç»†åŒ–å")

            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            report_path = pipeline.generate_quality_report(
                str(original_file),
                str(output_file),
                result
            )

            if report_path:
                print(f"ğŸ“Š è´¨é‡æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")

            return True

        else:
            print(f"âŒ å¤„ç†å¤±è´¥: {result.get('error', 'unknown error')}")
            return False

    except Exception as e:
        print(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def compare_results():
    """æ¯”è¾ƒåŸå§‹é…ç½®å’Œä¼˜åŒ–é…ç½®çš„ç»“æœ"""
    saved_maps_dir = Path("/home/tianyu/codes/Mid360map/saved_maps")

    # æŸ¥æ‰¾æ–‡ä»¶
    original_files = list(saved_maps_dir.glob("*[!_refined].pcd"))
    if not original_files:
        print("âŒ æœªæ‰¾åˆ°åŸå§‹æ–‡ä»¶")
        return

    original_file = max(original_files, key=os.path.getmtime)
    old_refined_file = saved_maps_dir / f"{original_file.stem}_refined.pcd"
    new_refined_file = saved_maps_dir / f"{original_file.stem}_optimized_refined.pcd"

    print("ğŸ” å¯¹æ¯”åˆ†æç»“æœ:")
    print("=" * 50)

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    files_to_analyze = [
        (original_file, "åŸå§‹åœ°å›¾"),
    ]

    if old_refined_file.exists():
        files_to_analyze.append((old_refined_file, "åŸé…ç½®ç²¾ç»†åŒ–"))

    if new_refined_file.exists():
        files_to_analyze.append((new_refined_file, "ä¼˜åŒ–é…ç½®ç²¾ç»†åŒ–"))

    # åˆ†ææ¯ä¸ªæ–‡ä»¶
    for file_path, name in files_to_analyze:
        analyze_point_density_distribution(str(file_path), name)

    # å¯¹æ¯”æ€»ç»“
    print("\nğŸ“Š å¯¹æ¯”æ€»ç»“:")
    print("=" * 50)
    if old_refined_file.exists() and new_refined_file.exists():
        old_pcd = o3d.io.read_point_cloud(str(old_refined_file))
        new_pcd = o3d.io.read_point_cloud(str(new_refined_file))
        original_pcd = o3d.io.read_point_cloud(str(original_file))

        print(f"åŸå§‹ç‚¹æ•°: {len(original_pcd.points):,}")
        print(f"åŸé…ç½®ç»“æœ: {len(old_pcd.points):,} ç‚¹")
        print(f"ä¼˜åŒ–é…ç½®ç»“æœ: {len(new_pcd.points):,} ç‚¹")

        old_change = len(old_pcd.points) - len(original_pcd.points)
        new_change = len(new_pcd.points) - len(original_pcd.points)

        print(f"åŸé…ç½®ç‚¹æ•°å˜åŒ–: {old_change:+,} ({old_change/len(original_pcd.points)*100:+.2f}%)")
        print(f"ä¼˜åŒ–é…ç½®ç‚¹æ•°å˜åŒ–: {new_change:+,} ({new_change/len(original_pcd.points)*100:+.2f}%)")

        if abs(new_change) < abs(old_change):
            print("âœ… ä¼˜åŒ–é…ç½®å‡å°‘äº†ä¸å¿…è¦çš„ç‚¹æ•°å˜åŒ–")
        else:
            print("âš ï¸  éœ€è¦è¿›ä¸€æ­¥è°ƒæ•´é…ç½®å‚æ•°")

def main():
    parser = argparse.ArgumentParser(description="æµ‹è¯•ä¼˜åŒ–åçš„åœ°å›¾ç²¾ç»†åŒ–é…ç½®")
    parser.add_argument('--test', action='store_true', help='è¿è¡Œç²¾ç»†åŒ–æµ‹è¯•')
    parser.add_argument('--compare', action='store_true', help='å¯¹æ¯”åˆ†æç»“æœ')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œå®Œæ•´æµ‹è¯•å’Œå¯¹æ¯”')

    args = parser.parse_args()

    if args.all:
        print("ğŸš€ å¼€å§‹å®Œæ•´æµ‹è¯•æµç¨‹...")
        success = test_optimized_refinement()
        if success:
            compare_results()
    elif args.test:
        test_optimized_refinement()
    elif args.compare:
        compare_results()
    else:
        print("è¯·æŒ‡å®šæµ‹è¯•æ¨¡å¼: --test, --compare, æˆ– --all")

if __name__ == "__main__":
    main()